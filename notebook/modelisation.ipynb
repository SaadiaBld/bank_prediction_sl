{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
        "from xgboost import XGBRegressor\n",
        "from scipy.stats import randint\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, classification_report, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['LoanNr_ChkDgt', 'Name', 'City', 'State', 'Zip', 'Bank', 'BankState',\n",
              "       'NAICS', 'ApprovalDate', 'ApprovalFY', 'Term', 'NoEmp', 'NewExist',\n",
              "       'CreateJob', 'RetainedJob', 'FranchiseCode', 'UrbanRural', 'RevLineCr',\n",
              "       'LowDoc', 'ChgOffDate', 'DisbursementDate', 'DisbursementGross',\n",
              "       'BalanceGross', 'MIS_Status', 'ChgOffPrinGr', 'GrAppv', 'SBA_Appv'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importer le DataFrame propre depuis le fichier CSV\n",
        "df = pd.read_csv('data.csv')\n",
        "df.head(4)\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LoanNr_ChkDgt          int64\n",
              "Name                  object\n",
              "City                  object\n",
              "State                 object\n",
              "Zip                    int64\n",
              "Bank                  object\n",
              "BankState             object\n",
              "NAICS                  int64\n",
              "ApprovalDate          object\n",
              "ApprovalFY           float64\n",
              "Term                   int64\n",
              "NoEmp                  int64\n",
              "NewExist               int64\n",
              "CreateJob              int64\n",
              "RetainedJob            int64\n",
              "FranchiseCode          int64\n",
              "UrbanRural             int64\n",
              "RevLineCr             object\n",
              "LowDoc                object\n",
              "ChgOffDate            object\n",
              "DisbursementDate      object\n",
              "DisbursementGross    float64\n",
              "BalanceGross         float64\n",
              "MIS_Status             int64\n",
              "ChgOffPrinGr         float64\n",
              "GrAppv               float64\n",
              "SBA_Appv             float64\n",
              "NAICS_digit           object\n",
              "dtype: object"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remplacer les valeurs non numériques par 0 et le type des variables\n",
        "df['NewExist'] = df['NewExist'].fillna(0)\n",
        "df['UrbanRural'] = df['UrbanRural'].fillna(0)\n",
        "\n",
        "\n",
        "# Convertir la colonne en type entier\n",
        "df['NewExist'] = df['NewExist'].astype(int)\n",
        "df['NewExist'].astype(int)\n",
        "\n",
        "df['UrbanRural'] = df['UrbanRural'].astype(int)\n",
        "df['UrbanRural'].astype(int)\n",
        "\n",
        "#conserver les deux premiers chiffres de naics\n",
        "df['NAICS_digit'] = df['NAICS'].astype(str).str[:2]\n",
        "\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre de lignes : 897167\n",
            "Nombre de colonnes : 28\n"
          ]
        }
      ],
      "source": [
        "#Supprimer les colonnes inutiles\n",
        "#df = df.drop(['ApprovalFY', 'ApprovalDate'], axis=1)\n",
        "\n",
        "#traiter la colonen NAICS pour qu'elle ne contienne que les 2 premiers chiffres des valeurs NAICS\n",
        "#df['NAICS_digit'] = (df['NAICS'] / 10000 ).astype(int)\n",
        "#df = df.drop(['NAICS'], axis=1)\n",
        "df.columns\n",
        "# Utilisez l'attribut shape pour obtenir le nombre de lignes et de colonnes de votre dataframe\n",
        "nombre_lignes, nombre_colonnes = df.shape\n",
        "\n",
        "print(\"Nombre de lignes :\", nombre_lignes)\n",
        "print(\"Nombre de colonnes :\", nombre_colonnes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modélisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "on teste une modélisation avec une imputation des valeurs manquantes, et une en supprimant les valeurs manquantes du df. Selon les résultats, on optera pour l'une ou l'autre du choix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. RandomForest Simple avec SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['LoanNr_ChkDgt', 'Name', 'City', 'State', 'Zip', 'Bank', 'BankState',\n",
              "       'NAICS', 'ApprovalDate', 'ApprovalFY', 'Term', 'NoEmp', 'NewExist',\n",
              "       'CreateJob', 'RetainedJob', 'FranchiseCode', 'UrbanRural', 'RevLineCr',\n",
              "       'LowDoc', 'ChgOffDate', 'DisbursementDate', 'DisbursementGross',\n",
              "       'BalanceGross', 'MIS_Status', 'ChgOffPrinGr', 'GrAppv', 'SBA_Appv',\n",
              "       'NAICS_digit'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Visualisation\n",
        "from sklearn.tree import export_graphviz\n",
        "from IPython.display import Image\n",
        "import graphviz\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#separer dataset en features et target\n",
        "X = df.drop('MIS_Status', axis=1)\n",
        "y = df['MIS_Status']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, test_size=0.05, random_state=42, stratify= y)\n",
        "\n",
        "cat_col = ['RevLineCr', 'LowDoc', 'FranchiseCode', 'UrbanRural', 'NewExist']\n",
        "num_col = ['NAICS_digit', 'Term' ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "A given column is not a column of the dataframe",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'NAICS_digit'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py:448\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 448\u001b[0m     col_idx \u001b[38;5;241m=\u001b[39m \u001b[43mall_columns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'NAICS_digit'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Création du pipeline : prétraitement + modèle RandomForest\u001b[39;00m\n\u001b[1;32m     19\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[1;32m     20\u001b[0m                            (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m, RandomForestClassifier())])\n\u001b[0;32m---> 22\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# performance du modèle\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py:401\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    400\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 401\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py:359\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    357\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/joblib/memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 893\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:724\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[0;32m--> 724\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m    727\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:426\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    424\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[1;32m    425\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[0;32m--> 426\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/sklearn/utils/__init__.py:456\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    453\u001b[0m             column_indices\u001b[38;5;241m.\u001b[39mappend(col_idx)\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA given column is not a column of the dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mValueError\u001b[0m: A given column is not a column of the dataframe"
          ]
        }
      ],
      "source": [
        "# Test RandomForest sans hyperparamètres \n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop= 'if_binary'))\n",
        "])\n",
        "\n",
        "num_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', cat_transformer, cat_col),\n",
        "        ('num', num_transformer, num_col)\n",
        "    ])\n",
        "\n",
        "# Création du pipeline : prétraitement + modèle RandomForest\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', RandomForestClassifier())])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# performance du modèle\n",
        "score_tr = pipeline.score(X_train, y_train)\n",
        "score_te = pipeline.score(X_test, y_test)\n",
        "\n",
        "# importance des caractéristiques\n",
        "feature_importance = pipeline.named_steps['classifier'].feature_importances_\n",
        "print(\"Importance des caractéristiques :\", feature_importance)\n",
        "\n",
        "print('_____________________')\n",
        "print(\"Score du modèle (train) :\", score_tr)\n",
        "print(\"Score du modèle (test) :\", score_te)\n",
        "print('_____________________')\n",
        "\n",
        "#matrice confusion\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print('_____________________')\n",
        "\n",
        "print(\"Métrique pour le modèle RandomForest Simple\")\n",
        "print(\"Score d'accuracy\", accuracy_score(y_test, y_pred))\n",
        "print(\"Score du recall : \", recall_score(y_test, y_pred))\n",
        "print(\"Score de la precision : \", precision_score(y_test, y_pred))\n",
        "print(\"Score F1 : \", f1_score(y_test, y_pred))\n",
        "print(\"\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#on supprime les valeurs manquantes nan de notre df puis on applique une modélisation\n",
        "\n",
        "missing_rows = df.isnull().any(axis=1).sum()\n",
        "missing_rows\n",
        "\n",
        "df_sans_valeurs_manquantes = df.dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "#refait modelisation sur df_sans_valeurs_manquantes sans valeurs manquantes\n",
        "#separer dataset en features et target\n",
        "X = df_sans_valeurs_manquantes.drop('MIS_Status', axis=1)\n",
        "y = df_sans_valeurs_manquantes['MIS_Status']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, test_size=0.05, random_state=42, stratify= y)\n",
        "\n",
        "cat_col = ['RevLineCr', 'LowDoc', 'FranchiseCode', 'UrbanRural', 'NewExist']\n",
        "num_col = ['NAICS_digit', 'Term' ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Importance des caractéristiques : [1.21888568e-02 5.62935609e-03 7.35786542e-03 3.13994680e-03\n",
            " 3.13834968e-03 3.36263484e-03 4.61852904e-04 2.70303161e-03\n",
            " 2.88109041e-03 2.55470815e-01 7.03666201e-01]\n",
            "_____________________\n",
            "Score du modèle (train) : 0.9738984054164046\n",
            "Score du modèle (test) : 0.9690739764540502\n",
            "_____________________\n",
            "_____________________\n",
            "Métrique pour le modèle RandomForest Simple sur df sans valeurs manquantes\n",
            "Score d'accuracy 0.9690739764540502\n",
            "Score du recall :  0.9972870320130223\n",
            "Score de la precision :  0.9716299559471365\n",
            "Score F1 :  0.9842913245269547\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.06      0.01      0.01       162\n",
            "           1       0.97      1.00      0.98      5529\n",
            "\n",
            "    accuracy                           0.97      5691\n",
            "   macro avg       0.52      0.50      0.50      5691\n",
            "weighted avg       0.95      0.97      0.96      5691\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test RandomForest sans hyperparamètres \n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop= 'if_binary'))\n",
        "])\n",
        "\n",
        "num_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', cat_transformer, cat_col),\n",
        "        ('num', num_transformer, num_col)\n",
        "    ])\n",
        "\n",
        "# Création du pipeline : prétraitement + modèle RandomForest\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', RandomForestClassifier())])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# performance du modèle\n",
        "score_tr = pipeline.score(X_train, y_train)\n",
        "score_te = pipeline.score(X_test, y_test)\n",
        "\n",
        "# importance des caractéristiques\n",
        "feature_importance = pipeline.named_steps['classifier'].feature_importances_\n",
        "print(\"Importance des caractéristiques :\", feature_importance)\n",
        "\n",
        "print('_____________________')\n",
        "print(\"Score du modèle (train) :\", score_tr)\n",
        "print(\"Score du modèle (test) :\", score_te)\n",
        "print('_____________________')\n",
        "\n",
        "#matrice confusion\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print('_____________________')\n",
        "\n",
        "print(\"Métrique pour le modèle RandomForest Simple sur df sans valeurs manquantes\")\n",
        "print(\"Score d'accuracy\", accuracy_score(y_test, y_pred))\n",
        "print(\"Score du recall : \", recall_score(y_test, y_pred))\n",
        "print(\"Score de la precision : \", precision_score(y_test, y_pred))\n",
        "print(\"Score F1 : \", f1_score(y_test, y_pred))\n",
        "print(\"\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.Testons KNNImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score du modèle (train) : 0.9467575101958446\n",
            "Score du modèle (test) : 0.927104928776834\n",
            "Métrique pour le modèle AdaboostClassifierr\n",
            "Score d'accuracy 0.927104928776834\n",
            "Score du recall :  0.764026402640264\n",
            "Score de la precision :  0.8100942126514132\n",
            "Score F1 :  0.7863862032923961\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.96      0.96     36981\n",
            "           1       0.81      0.76      0.79      7878\n",
            "\n",
            "    accuracy                           0.93     44859\n",
            "   macro avg       0.88      0.86      0.87     44859\n",
            "weighted avg       0.93      0.93      0.93     44859\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test KNNImputer\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop = 'if_binary'))\n",
        "])\n",
        "\n",
        "num_transformer = Pipeline(steps=[\n",
        "    ('imputer', KNNImputer(n_neighbors = 3, weights = 'uniform')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', cat_transformer, cat_col),\n",
        "        ('num', num_transformer, num_col)\n",
        "    ])\n",
        "\n",
        "# Création du pipeline : prétraitement + modèle RandomForest\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', RandomForestClassifier())])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# performance du modèle\n",
        "score_tr = pipeline.score(X_train, y_train)\n",
        "score_te = pipeline.score(X_test, y_test)\n",
        "\n",
        "print(\"Score du modèle (train) :\", score_tr)\n",
        "print(\"Score du modèle (test) :\", score_te)\n",
        "\n",
        "# matrice confusion\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Métrique pour le modèle AdaboostClassifierr\")\n",
        "print(\"Score d'accuracy\", accuracy_score(y_test, y_pred))\n",
        "print(\"Score du recall : \", recall_score(y_test, y_pred))\n",
        "print(\"Score de la precision : \", precision_score(y_test, y_pred))\n",
        "print(\"Score F1 : \", f1_score(y_test, y_pred))\n",
        "print(\"\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score du modèle (train) : 0.8975565050467521\n",
            "Score du modèle (test) : 0.8926178985030707\n"
          ]
        }
      ],
      "source": [
        "# Tests KNNImputer pour les variables numeriques\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop = 'if_binary'))\n",
        "])\n",
        "\n",
        "num_transformer = Pipeline(steps=[\n",
        "    ('imputer', KNNImputer(n_neighbors = 3, weights = 'uniform')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', cat_transformer, cat_col),\n",
        "        ('num', num_transformer, num_col)\n",
        "    ])\n",
        "\n",
        "# Création du pipeline : prétraitement + modèle RandomForest\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', RandomForestClassifier(n_estimators=80, max_depth=40, min_samples_leaf=5, max_features='sqrt'))])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# performance du modèle\n",
        "score_tr = pipeline.score(X_train, y_train)\n",
        "score_te = pipeline.score(X_test, y_test)\n",
        "\n",
        "print(\"Score du modèle (train) :\", score_tr)\n",
        "print(\"Score du modèle (test) :\", score_te)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/utilisateur/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [2] during transform. These unknown categories will be encoded as all zeros\n",
            "  warnings.warn(\n",
            "/home/utilisateur/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [2] during transform. These unknown categories will be encoded as all zeros\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score du modèle (train) : 0.8981764808027145\n",
            "Score du modèle (test) : 0.8956508170043915\n",
            "Métrique pour le modèle AdaboostClassifierr\n",
            "Score d'accuracy 0.8956508170043915\n",
            "Score du recall :  0.46585427773546584\n",
            "Score de la precision :  0.8858315230509293\n",
            "Score F1 :  0.6105981199567423\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.99      0.94     36981\n",
            "           1       0.89      0.47      0.61      7878\n",
            "\n",
            "    accuracy                           0.90     44859\n",
            "   macro avg       0.89      0.73      0.78     44859\n",
            "weighted avg       0.89      0.90      0.88     44859\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#on joue avec les hyperparametre\n",
        "\n",
        "# Tests KNNImputer pour les variables numeriques\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop= 'if_binary'))\n",
        "])\n",
        "\n",
        "num_transformer = Pipeline(steps=[\n",
        "    ('imputer', IterativeImputer (max_iter=10, random_state=0)),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', cat_transformer, cat_col),\n",
        "        ('num', num_transformer, num_col)\n",
        "    ])\n",
        "\n",
        "# Création du pipeline : prétraitement + modèle RandomForest\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', RandomForestClassifier(n_estimators= 80, max_depth=40, min_samples_leaf=5, max_features='sqrt'))])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "\n",
        "# performance du modèle\n",
        "score_tr = pipeline.score(X_train, y_train)\n",
        "score_te = pipeline.score(X_test, y_test)\n",
        "\n",
        "print(\"Score du modèle (train) :\", score_tr)\n",
        "print(\"Score du modèle (test) :\", score_te)\n",
        "\n",
        "# matrice confusion\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Métrique pour le modèle AdaboostClassifierr\")\n",
        "print(\"Score d'accuracy\", accuracy_score(y_test, y_pred))\n",
        "print(\"Score du recall : \", recall_score(y_test, y_pred))\n",
        "print(\"Score de la precision : \", precision_score(y_test, y_pred))\n",
        "print(\"Score F1 : \", f1_score(y_test, y_pred))\n",
        "print(\"\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modélisation \n",
        "\n",
        "#### 1. Randomized search + RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['LoanNr_ChkDgt', 'Name', 'City', 'State', 'Zip', 'Bank', 'BankState',\n",
              "       'NAICS', 'ApprovalDate', 'ApprovalFY', 'Term', 'NoEmp', 'NewExist',\n",
              "       'CreateJob', 'RetainedJob', 'FranchiseCode', 'UrbanRural', 'RevLineCr',\n",
              "       'LowDoc', 'ChgOffDate', 'DisbursementDate', 'DisbursementGross',\n",
              "       'BalanceGross', 'MIS_Status', 'ChgOffPrinGr', 'GrAppv', 'SBA_Appv',\n",
              "       'NAICS_digit'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#separer dataset en features et target\n",
        "X = df.drop('MIS_Status', axis=1)\n",
        "y = df['MIS_Status']\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, test_size=0.05, random_state=42, stratify= y)\n",
        "\n",
        "cat_col = ['RevLineCr', 'LowDoc', 'FranchiseCode', 'NAICS_digit', 'NewExist', 'UrbanRural' ]\n",
        "num_col = ['Term', 'CreateJob', 'RetainedJob', 'SBA_Appv', 'GrAppv']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "num_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# random search cv\n",
        "hyper_grid = {'classifier__max_depth':list(np.arange(10, 100, step=10)) + [30],\n",
        "              'classifier__n_estimators':[100],\n",
        "              'classifier__max_features':randint(1,7),\n",
        "              'classifier__min_samples_leaf':randint(1,4),\n",
        "              'classifier__min_samples_split':np.arange(2, 10, step=2)\n",
        "          }\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', cat_transformer, cat_col),\n",
        "        ('num', num_transformer, num_col)\n",
        "    ])\n",
        "\n",
        "# Création du pipeline : prétraitement + modèle RandomForest\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', RandomForestClassifier())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "random_cv = RandomizedSearchCV(estimator= pipeline,\n",
        "                               param_distributions=hyper_grid,\n",
        "                               cv = 3,\n",
        "                               n_iter= 9,\n",
        "                               scoring = 'accuracy',\n",
        "                               n_jobs= None,\n",
        "                               return_train_score = True,\n",
        "                               random_state = 42)\n",
        "\n",
        "random_cv.fit(X_train, y_train)\n",
        "\n",
        "# Afficher les meilleurs paramètres\n",
        "print(\"Meilleurs paramètres:\", random_cv.best_params_)\n",
        "\n",
        "# Performance du meilleur modèle trouvé\n",
        "score_tr = random_cv.best_estimator_.score(X_train, y_train)\n",
        "score_te = random_cv.best_estimator_.score(X_test, y_test)\n",
        "\n",
        "print(\"Score du modèle (train) :\", score_tr)\n",
        "print(\"Score du modèle (test) :\", score_te)\n",
        "\n",
        "\n",
        "# #modele sans randomizedsearchcv\n",
        "# #pipeline.fit(X_train, y_train)\n",
        "\n",
        "# # performance du modèle\n",
        "# score_tr = pipeline.score(X_train, y_train)\n",
        "# score_te = pipeline.score(X_test, y_test)\n",
        "\n",
        "# print(\"Score du modèle (train) :\", score_tr)\n",
        "# print(\"Score du modèle (test) :\", score_te)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RandomizedSearchCV donne les résultats suivants:\n",
        "Meilleurs paramètres: {'classifier__max_depth': 70, 'classifier__max_features': 4, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 6, 'classifier__n_estimators': 100}\n",
        "Score du modèle (train) : 0.824524119140504\n",
        "Score du modèle (test) : 0.8231104472953844\n",
        "\n",
        "On peut restreindre la recherche d'hyperparamètres à des valeurs proches de ces résultats dans la gridsearchcv.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "# on garde la même configuration des X et y.\n",
        "\n",
        "# GridSearchCv\n",
        "params = {'classifier__max_depth': [60, 70, 80],\n",
        "              'classifier__n_estimators':[90, 100, 110],\n",
        "              'classifier__max_features': [3, 4, 5],\n",
        "              'classifier__min_samples_leaf':[1,2],\n",
        "              'classifier__min_samples_split': [5,6,7]\n",
        "          }\n",
        "\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', cat_transformer, cat_col),\n",
        "        ('num', num_transformer, num_col)\n",
        "    ])\n",
        "\n",
        "# Création du pipeline : prétraitement + modèle RandomForest\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', RandomForestClassifier())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid = GridSearchCV(pipeline, param_grid = params, scoring = 'accuracy', cv = 4)\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Meilleurs paramètres de GridSearch:\", grid.best_params_)\n",
        "\n",
        "\n",
        "# Performance du meilleur modèle trouvé\n",
        "score_tr = grid.best_estimator_.score(X_train, y_train)\n",
        "score_te = grid.best_estimator_.score(X_test, y_test)\n",
        "\n",
        "print(\"Score du modèle (train) :\", score_tr)\n",
        "print(\"Score du modèle (test) :\", score_te)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparamétres du modéle\n",
        "\n",
        "* Random Forest\n",
        "     - max_depth\n",
        "     - min_sample_split\n",
        "     - max_leaf_nodes\n",
        "     - min_samples_leaf\n",
        "     - n_estimators\n",
        "     - max_sample (bootstrap sample)\n",
        "     - max_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# random search cv\n",
        "hyper_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['mean', 'median'],  # Ajoutez les stratégies d'imputation numérique ici si nécessaire\n",
        "    'classifier_max_depth': list(np.arange(10, 100, step=10)) + [None],\n",
        "    'classifier__n_estimators': np.arange(10, 500, step=50),\n",
        "    'classifier__max_features': randint(1, 7),\n",
        "    'classifier__criterion': ['gini', 'entropy'],\n",
        "    'classifier__min_samples_leaf': randint(1, 4),\n",
        "    'classifier__min_samples_split': np.arange(2, 10, step=2)\n",
        "}\n",
        "\n",
        "# Création du preprocessor pour gérer les deux types de colonnes\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', cat_transformer, cat_col),\n",
        "        ('num', num_transformer, num_col)\n",
        "    ])\n",
        "\n",
        "# Création du pipeline incluant le prétraitement et le modèle RandomForest\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', RandomForestClassifier())])\n",
        "\n",
        "random_cv = RandomizedSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_distributions=hyper_grid,\n",
        "    cv=3,\n",
        "    n_iter=5,\n",
        "    scoring='accuracy',  # ou une autre métrique que vous souhaitez évaluer\n",
        "    n_jobs=-1,\n",
        "    return_train_score=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit du RandomizedSearchCV\n",
        "random_cv.fit(X_train, y_train)\n",
        "\n",
        "# Afficher les meilleurs paramètres\n",
        "print(\"Meilleurs paramètres:\", random_cv.best_params_)\n",
        "\n",
        "# Performance du meilleur modèle trouvé\n",
        "score_tr = random_cv.best_estimator_.score(X_train, y_train)\n",
        "score_te = random_cv.best_estimator_.score(X_test, y_test)\n",
        "\n",
        "print(\"Score du modèle (train) :\", score_tr)\n",
        "print(\"Score du modèle (test) :\", score_te)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Adaboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#adaboost\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
        "\n",
        "#separer dataset en features et target\n",
        "X = df.drop('MIS_Status', axis=1)\n",
        "y = df['MIS_Status']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, test_size=0.05, random_state=42, stratify=y)\n",
        "\n",
        "cat_col = ['RevLineCr', 'LowDoc', 'FranchiseCode' ]\n",
        "num_col = ['NAICS_digit', 'Term', 'NewExist', 'UrbanRural']\n",
        "\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "num_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler()),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score du modèle (train) : 0.8935711189547341\n",
            "Score du modèle (test) : 0.894044606930682\n",
            "Métrique pour le modèle XGBClassifier\n",
            "Score d'accuracy 0.894044606930682\n",
            "Score du recall :  0.5917454316320101\n",
            "Score de la precision :  0.7562409405701401\n",
            "Score F1 :  0.663956447963801\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.96      0.94     73847\n",
            "           1       0.76      0.59      0.66     15870\n",
            "\n",
            "    accuracy                           0.89     89717\n",
            "   macro avg       0.84      0.78      0.80     89717\n",
            "weighted avg       0.89      0.89      0.89     89717\n",
            "\n"
          ]
        }
      ],
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', cat_transformer, cat_col),\n",
        "        ('num', num_transformer, num_col)\n",
        "    ])\n",
        "\n",
        "# Création du pipeline : prétraitement + modèle RandomForest\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', AdaBoostClassifier())])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# performance du modèle\n",
        "score_tr = pipeline.score(X_train, y_train)\n",
        "score_te = pipeline.score(X_test, y_test)\n",
        "\n",
        "print(\"Score du modèle (train) :\", score_tr)\n",
        "print(\"Score du modèle (test) :\", score_te)\n",
        "\n",
        "#matrice confusion\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Métrique pour le modèle AdaboostClassifierr\")\n",
        "print(\"Score d'accuracy\", accuracy_score(y_test, y_pred))\n",
        "print(\"Score du recall : \", recall_score(y_test, y_pred))\n",
        "print(\"Score de la precision : \", precision_score(y_test, y_pred))\n",
        "print(\"Score F1 : \", f1_score(y_test, y_pred))\n",
        "print(\"\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/utilisateur/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [2] during transform. These unknown categories will be encoded as all zeros\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score du modèle (train) : 0.930283418670246\n",
            "Score du modèle (test) : 0.9302035266055864\n",
            "Métrique pour le modèle XGBClassifier\n",
            "Score d'accuracy 0.9302035266055864\n",
            "Score du recall :  0.765041888804265\n",
            "Score de la precision :  0.8248255097851376\n",
            "Score F1 :  0.793809680605861\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     36981\n",
            "           1       0.82      0.77      0.79      7878\n",
            "\n",
            "    accuracy                           0.93     44859\n",
            "   macro avg       0.89      0.87      0.88     44859\n",
            "weighted avg       0.93      0.93      0.93     44859\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/utilisateur/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [2] during transform. These unknown categories will be encoded as all zeros\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop = 'if_binary'))\n",
        "])\n",
        "\n",
        "num_transformer = Pipeline(steps=[\n",
        "    ('imputer', KNNImputer(n_neighbors = 3, weights = 'distance')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', cat_transformer, cat_col),\n",
        "        ('num', num_transformer, num_col)\n",
        "    ])\n",
        "\n",
        "# Création du pipeline : prétraitement + modèle XGBoost\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', XGBClassifier(random_state=42))])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# performance du modèle\n",
        "score_tr = pipeline.score(X_train, y_train)\n",
        "score_te = pipeline.score(X_test, y_test)\n",
        "\n",
        "print(\"Score du modèle (train) :\", score_tr)\n",
        "print(\"Score du modèle (test) :\", score_te)\n",
        "\n",
        "#matrice confusion\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Métrique pour le modèle XGBClassifier\")\n",
        "print(\"Score d'accuracy\", accuracy_score(y_test, y_pred))\n",
        "print(\"Score du recall : \", recall_score(y_test, y_pred))\n",
        "print(\"Score de la precision : \", precision_score(y_test, y_pred))\n",
        "print(\"Score F1 : \", f1_score(y_test, y_pred))\n",
        "print(\"\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rappel de l'encodage pour la variable MIS_Status: {'P I F': 0, 'CHGOFF': 1}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "#separer dataset en features et target\n",
        "X = df_cat.drop('MIS_Status', axis=1)\n",
        "y = df_cat['MIS_Status']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, test_size=0.05, random_state=42, stratify= y)\n",
        "\n",
        "cat_col = ['RevLineCr', 'LowDoc', 'FranchiseCode', 'UrbanRural']\n",
        "num_col = ['NAICS_digit', 'Term', 'SBA_Appv', 'GrAppv']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Term               int64\n",
              "FranchiseCode      int64\n",
              "UrbanRural         int64\n",
              "RevLineCr         object\n",
              "LowDoc            object\n",
              "MIS_Status         int64\n",
              "GrAppv           float64\n",
              "SBA_Appv         float64\n",
              "NAICS_digit       object\n",
              "dtype: object"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cat = df.drop(['LoanNr_ChkDgt', 'Name','NAICS', 'City', 'State', 'Zip', 'Bank', 'BankState', 'ApprovalDate', 'ApprovalFY', 'NoEmp', 'CreateJob', 'RetainedJob', 'ChgOffDate', 'DisbursementDate', 'DisbursementGross', 'BalanceGross', 'ChgOffPrinGr', 'NewExist'], axis=1)\n",
        "df_cat.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score du modèle (train) : 0.9407315195915091\n",
            "Score du modèle (test) : 0.9381840879199269\n",
            "Noms des variables et leur importance : {'RevLineCr_Y': 'Feature Id', 'LowDoc_Y': 'Importances'}\n",
            "Métrique pour le modèle XGBClassifier\n",
            "Score d'accuracy 0.9381840879199269\n",
            "Score du recall :  0.8010916476263011\n",
            "Score de la precision :  0.8395636557137156\n",
            "Score F1 :  0.8198765833062682\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.96     36981\n",
            "           1       0.84      0.80      0.82      7878\n",
            "\n",
            "    accuracy                           0.94     44859\n",
            "   macro avg       0.90      0.88      0.89     44859\n",
            "weighted avg       0.94      0.94      0.94     44859\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from catboost import CatBoostClassifier\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop = 'if_binary'))\n",
        "])\n",
        "\n",
        "num_transformer = Pipeline(steps=[\n",
        "    ('imputer', KNNImputer(n_neighbors = 2, weights = 'uniform')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', cat_transformer, cat_col),\n",
        "        ('num', num_transformer, num_col)\n",
        "    ])\n",
        "\n",
        "# Création du pipeline : prétraitement + modèle CatBoost\n",
        "modelcat = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', CatBoostClassifier(random_state=42, verbose=False))])\n",
        "\n",
        "modelcat.fit(X_train, y_train)\n",
        "\n",
        "y_pred = modelcat.predict(X_test)\n",
        "\n",
        "# performance du modèle\n",
        "score_tr = modelcat.score(X_train, y_train)\n",
        "score_te = modelcat.score(X_test, y_test)\n",
        "\n",
        "print(\"Score du modèle (train) :\", score_tr)\n",
        "print(\"Score du modèle (test) :\", score_te)\n",
        "\n",
        "#matrice confusion\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "#extrait classifieur de la pipeline\n",
        "catboost_classifier = modelcat.named_steps['classifier']\n",
        "#feature importance\n",
        "feature_importance = catboost_classifier.get_importance(prettified=True)\n",
        "print('importance des features: feature_importance)\n",
        "#feature_importance = pipeline.named_steps['classifier'].get_feature_importance()\n",
        "\n",
        "print(\"Métrique pour le modèle XGBClassifier\")\n",
        "print(\"Score d'accuracy\", accuracy_score(y_test, y_pred))\n",
        "print(\"Score du recall : \", recall_score(y_test, y_pred))\n",
        "print(\"Score de la precision : \", precision_score(y_test, y_pred))\n",
        "print(\"Score F1 : \", f1_score(y_test, y_pred))\n",
        "print(\"\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Noms des variables : ['RevLineCr_Y', 'LowDoc_Y', 'FranchiseCode_1', 'UrbanRural_0', 'UrbanRural_1', 'UrbanRural_2', 'NewExist_0', 'NewExist_1', 'NewExist_2', 'NAICS_digit', 'Term', 'SBA_Appv', 'GrAppv']\n"
          ]
        }
      ],
      "source": [
        "# Obtenez les noms de colonnes transformées\n",
        "preprocessed_cat_columns = modelcat.named_steps['preprocessor'].transformers_[0][1]['onehot'].get_feature_names_out(input_features=cat_col)\n",
        "preprocessed_columns = list(preprocessed_cat_columns) + num_col\n",
        "\n",
        "print('Noms des variables :', preprocessed_columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from catboost import CatBoostClassifier\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop = 'if_binary'))\n",
        "])\n",
        "\n",
        "num_transformer = Pipeline(steps=[\n",
        "    ('imputer', KNNImputer(n_neighbors = 2, weights = 'uniform')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', cat_transformer, cat_col),\n",
        "        ('num', num_transformer, num_col)\n",
        "    ])\n",
        "\n",
        "# Création du pipeline : prétraitement + modèle CatBoost\n",
        "modelcat = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', CatBoostClassifier(random_state=42, verbose=False))])\n",
        "\n",
        "modelcat.fit(X_train, y_train)\n",
        "\n",
        "y_pred = modelcat.predict(X_test)\n",
        "\n",
        "# performance du modèle\n",
        "score_tr = modelcat.score(X_train, y_train)\n",
        "score_te = modelcat.score(X_test, y_test)\n",
        "\n",
        "print(\"Score du modèle (train) :\", score_tr)\n",
        "print(\"Score du modèle (test) :\", score_te)\n",
        "\n",
        "#matrice confusion\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# #extrait classifieur de la pipeline\n",
        "# catboost_classifier = modelcat.named_steps['classifier']\n",
        "# #feature importance\n",
        "# feature_importance = catboost_classifier.get_feature_importance(prettified=True)\n",
        "# print('importance des features:', feature_importance)\n",
        "# #feature_importance = pipeline.named_steps['classifier'].get_feature_importance()\n",
        "\n",
        "print(\"Métrique pour le modèle XGBClassifier\")\n",
        "print(\"Score d'accuracy\", accuracy_score(y_test, y_pred))\n",
        "print(\"Score du recall : \", recall_score(y_test, y_pred))\n",
        "print(\"Score de la precision : \", precision_score(y_test, y_pred))\n",
        "print(\"Score F1 : \", f1_score(y_test, y_pred))\n",
        "print(\"\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "#le modele catboost est le plus performant pour l'instant, on le charge sous forme d'un fichier pkl\n",
        "import pickle\n",
        "\n",
        "pickle.dump(modelcat, open('modelcatb.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Term</th>\n",
              "      <th>FranchiseCode</th>\n",
              "      <th>UrbanRural</th>\n",
              "      <th>RevLineCr</th>\n",
              "      <th>LowDoc</th>\n",
              "      <th>MIS_Status</th>\n",
              "      <th>GrAppv</th>\n",
              "      <th>SBA_Appv</th>\n",
              "      <th>NAICS_digit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>60000.0</td>\n",
              "      <td>48000.0</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>40000.0</td>\n",
              "      <td>32000.0</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>180</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>287000.0</td>\n",
              "      <td>215250.0</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>35000.0</td>\n",
              "      <td>28000.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>240</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>0</td>\n",
              "      <td>229000.0</td>\n",
              "      <td>229000.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Term  FranchiseCode  UrbanRural RevLineCr LowDoc  MIS_Status    GrAppv  \\\n",
              "0    84              0           0         N      Y           0   60000.0   \n",
              "1    60              0           0         N      Y           0   40000.0   \n",
              "2   180              0           0         N      N           0  287000.0   \n",
              "3    60              0           0         N      Y           0   35000.0   \n",
              "4   240              0           0         N      N           0  229000.0   \n",
              "\n",
              "   SBA_Appv NAICS_digit  \n",
              "0   48000.0          45  \n",
              "1   32000.0          72  \n",
              "2  215250.0          62  \n",
              "3   28000.0           0  \n",
              "4  229000.0           0  "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_cat.columns\n",
        "df_cat.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        Term  FranchiseCode  UrbanRural RevLineCr LowDoc    GrAppv  SBA_Appv  \\\n",
            "0         84              0           0         N      Y   60000.0   48000.0   \n",
            "1         60              0           0         N      Y   40000.0   32000.0   \n",
            "2        180              0           0         N      N  287000.0  215250.0   \n",
            "3         60              0           0         N      Y   35000.0   28000.0   \n",
            "4        240              0           0         N      N  229000.0  229000.0   \n",
            "...      ...            ...         ...       ...    ...       ...       ...   \n",
            "897162    60              0           0       NaN      N   70000.0   56000.0   \n",
            "897163    60              0           0         Y      N   85000.0   42500.0   \n",
            "897164   108              0           0         N      N  300000.0  225000.0   \n",
            "897165    60              0           0         N      Y   75000.0   60000.0   \n",
            "897166    48              0           0         N      N   30000.0   24000.0   \n",
            "\n",
            "       NAICS_digit  \n",
            "0               45  \n",
            "1               72  \n",
            "2               62  \n",
            "3                0  \n",
            "4                0  \n",
            "...            ...  \n",
            "897162          45  \n",
            "897163          45  \n",
            "897164          33  \n",
            "897165           0  \n",
            "897166           0  \n",
            "\n",
            "[897167 rows x 8 columns]\n"
          ]
        }
      ],
      "source": [
        "# values = {'Term':(120,100), 'FranchiseCode': (0,1), 'UrbanRural': (0,0), 'RevLineCr': ('N','Y'), 'LowDoc': ('Y','N'), 'GrAppv': (48000, 28000), 'SBA_Appv':(20000,60000), 'NAICS_digit':(45,62)}\n",
        "# pred = modelcat.predict(values)\n",
        "\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def predict_loan(model,data):\n",
        "    # Convertir la liste de listes en DataFrame\n",
        "    df = pd.DataFrame(data, columns=['Term', 'FranchiseCode', 'UrbanRural', 'RevLineCr', 'LowDoc', 'GrAppv', 'SBA_Appv', 'NAICS_digit'])\n",
        "    predictions = model.predict(df)\n",
        "    return predictions[0]\n",
        "\n",
        "predict_loan(modelcat,[[84, 0, 0, 'N', 'Y',60000.0, 48000.0, 45]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0         0\n",
              "1         0\n",
              "2         0\n",
              "3         0\n",
              "4         0\n",
              "         ..\n",
              "897162    0\n",
              "897163    0\n",
              "897164    0\n",
              "897165    1\n",
              "897166    0\n",
              "Name: MIS_Status, Length: 897167, dtype: int64"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(y[897166])\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stacking/Voting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
